= Spark SQL, DataFrames and Datasets Guide
:toc:
:toc-placement: preamble

== 1. Overview

Spark SQL is a Spark module for structured data processing. 

* Unlike the basic Spark RDD API, the interfaces provided by Spark SQL provide Spark with *more information* about the structure of both the *data* and the *computation* being performed. 
* Internally, Spark SQL uses this extra information to perform extra optimizations. 

There are several ways to interact with Spark SQL including *SQL* and *the Dataset API*. When computing a result the same *execution engine* is used, independent of which API/language you are using to express the computation. 

=== 1.1 SQL

* One use of Spark SQL is to execute SQL queries. 
* Spark SQL can also be used to read data from an existing Hive installation. For more on how to configure this feature, please refer to the Hive Tables section. 
* When running SQL from within another programming language the results will be returned as a Dataset/DataFrame. 
* You can also interact with the SQL interface using the command-line or over JDBC/ODBC.

=== 1.2 Datasets and DataFrames

A *_Dataset_* is a distributed collection of data. 

* Dataset is a new interface added in Spark 1.6 that provides the benefits of RDDs (strong typing, ability to use powerful lambda functions) with the benefits of Spark SQL’s optimized execution engine. 
* A Dataset can be constructed from JVM objects and then manipulated using functional transformations (map, flatMap, filter, etc.). 
* The *Dataset API* is available in *Scala* and *Java*. *Python* does not have the support for the Dataset API. But due to Python’s dynamic nature, many of the benefits of the Dataset API are already available (i.e. you can access the field of a row by name naturally row.columnName). The case for *R* is similar.

A *_DataFrame_* is a Dataset organized into named columns. 

* It is conceptually equivalent to *a table in a relational database* or *a data frame in R/Python*, but with richer optimizations under the hood. 
* DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs. 
* The *DataFrame API* is available in Scala, Java, Python, and R. In *Scala* and *Java*, a DataFrame is represented by *a Dataset of Rows*. In the Scala API, DataFrame is simply a type alias of Dataset[Row]. While, in Java API, users need to use Dataset<Row> to represent a DataFrame.

== 2. Getting Started

=== 2.1 Starting Point: SparkSession

```scala
import org.apache.spark.sql.SparkSession

val spark = SparkSession
  .builder()
  .appName("Spark SQL basic example")
  .config("spark.some.config.option", "some-value")
  .getOrCreate()

// For implicit conversions like converting RDDs to DataFrames
import spark.implicits._
```

SparkSession in Spark 2.0 provides builtin support for Hive features including:

* the ability to write queries using HiveQL, 
* access to Hive UDFs, and 
* the ability to read data from Hive tables. 

=== 2.2 Creating DataFrames

With a SparkSession, applications can create DataFrames:

* from an existing RDD, 
* from a Hive table, or 
* from Spark data sources.

=== 2.3 Untyped Dataset Operations (aka DataFrame Operations)

DataFrames provide a *domain-specific language* (DSL) for structured data manipulation in Scala, Java, Python and R.

As mentioned above, in Spark 2.0, DataFrames are just Dataset of Rows in Scala and Java API. These operations are also referred as “*untyped transformations*” in contrast to “*typed transformations*” come with strongly typed Scala/Java Datasets.

=== 2.4 Running SQL Queries Programmatically

The *_sql_* function on a *SparkSession* enables applications to run SQL queries programmatically and returns the result as a *DataFrame*.

=== 2.5 Global Temporary View

* Temporary views in Spark SQL are *session-scoped* and will disappear if the session that creates it terminates. 
* If you want to have a temporary view that is shared among all sessions and keep alive until the Spark application terminates, you can create a global temporary view. Global temporary view is tied to a system preserved database *global_temp*, and we must use the qualified name to refer it, e.g. SELECT * FROM global_temp.view1.

=== 2.6 Creating Datasets

Datasets are similar to RDDs, however, instead of using *Java serialization* or *Kryo* they use a specialized *Encoder* to serialize the objects for *processing* or *transmitting* over the network. 

While both encoders and standard serialization are responsible for turning an object *into bytes*, encoders are code generated *dynamically* and use a format that allows Spark to perform many operations like filtering, sorting and hashing *without deserializing* the bytes back into an object.

=== 2.7 Interoperating with RDDs

Spark SQL supports *two* different methods for converting existing RDDs into Datasets. 

* The first method uses reflection to infer the schema of an RDD that contains specific types of objects. This reflection based approach leads to more concise code and works well when you *already know the schema* while writing your Spark application.

* The second method for creating Datasets is through a programmatic interface that allows you to construct a schema and then apply it to an existing RDD. While this method is more verbose, it allows you to construct Datasets when *the columns and their types are not known* until runtime.

=== 2.8 Aggregations

* Untyped User-Defined Aggregate Functions: Users have to extend the *UserDefinedAggregateFunction* abstract class to implement a custom untyped aggregate function.

* Type-Safe User-Defined Aggregate Functions: User-defined aggregations for strongly typed Datasets revolve around the *Aggregator* abstract class. 


== 3. Data Sources

Spark SQL supports operating on a variety of data sources through the *DataFrame interface*. A DataFrame 

* can be operated on using *relational transformations* and 
* can also be used to create a *temporary view*. 
 ** Registering a DataFrame as a temporary view allows you to run *SQL queries* over its data. 

This section describes the general methods for *loading and saving* data using the Spark Data Sources and then goes into specific options that are available for the built-in data sources.

=== 3.1 Generic Load/Save Functions

=== 3.2 Parquet Files

=== 3.3 JSON Datasets

=== 3.4 Hive Tables

=== 3.5 JDBC To Other Databases

=== 3.6 Troubleshooting

== 4. Performance Tuning

=== 4.1  Data In Memory

=== 4.2 Other Configuration Options

== 5. Distributed SQL Engine

=== 5.1 Running the Thrift JDBC/ODBC serve4r

=== 5.2 Running the Spark SQL CLI

== 6. Migration Guide

== 7. Reference

=== 7.1 Data Types

=== 7.2 NaN Semantics
